{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "41a6777d5e67dc652f57ce9681b4c44dc44152be",
    "colab_type": "text",
    "id": "uNaVQGQ9tQRr"
   },
   "source": [
    "<h2 align=\"center\">CYFLOD</h2>\n",
    "\n",
    "<h3 align=\"center\">Date: 19 Jan 2024</h3>\n",
    "\n",
    "<h3 align=\"center\">Author: Nauman Ullah Gilal</h3>\n",
    "\n",
    "<h3 align=\"center\">Supervisor: Dr. Marco Agus</h3>\n",
    "\n",
    "<h3 align=\"center\">Institution: College of Science and Engineering,<br>Hamad Bin Khalifa University</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaration of Efficient Net family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.235114Z",
     "start_time": "2021-11-26T12:31:28.215051Z"
    }
   },
   "outputs": [],
   "source": [
    "architecture = 'efficientnet-b4'\n",
    "logfile = 'efficientnet-b0-MENA.csv'\n",
    "\n",
    "## The tables\n",
    "effnet_size = ({\n",
    "    'efficientnet-b0':300,      # original resolution\n",
    "    'efficientnet-b1':240,      # original resolution\n",
    "    'efficientnet-b2':260,      # original resolution\n",
    "    'efficientnet-b3':400,      # original resolution\n",
    "    'efficientnet-b4':500,      # original resolution\n",
    "    'efficientnet-b5':456,      # original resolution\n",
    "    'efficientnet-b6':528,      # original resolution\n",
    "    'efficientnet-b7':600,      # original resolution\n",
    "    'efficientnet-lite0':224,\n",
    "    'efficientnet-lite2':260,\n",
    "    'efficientnet-lite4':384,\n",
    "    'resnet18':224,\n",
    "    'resnet34':224,\n",
    "    'resnet50':224,\n",
    "    'resnet101':224,\n",
    "    'resnet152':224\n",
    "})\n",
    "batch_size = ({ \n",
    "    'efficientnet-b0':128, # bs = 256 for B1\n",
    "    'efficientnet-b1':120,      #\n",
    "    'efficientnet-b2':160,       #88 160\n",
    "    'efficientnet-b3':16,       #\n",
    "    'efficientnet-b4':32,       #\n",
    "    'efficientnet-b5':14,       #\n",
    "    'efficientnet-b6':16,\n",
    "    'efficientnet-b7':8,\n",
    "    'efficientnet-lite0': 160,\n",
    "    'efficientnet-lite2': 160,\n",
    "    'efficientnet-lite4':24,\n",
    "    \n",
    "    'resnet18':400,             # 3080: ok\n",
    "    'resnet34':320,            # RTX 3090 rocz!\n",
    "    'resnet50':256,             # 3080: ok\n",
    "    'resnet101':96,\n",
    "    'resnet152':64\n",
    "})\n",
    "learning_rate = ({\n",
    "    'efficientnet-b0':5e-4,     # 2e-3: too large, 5e-4: too large\n",
    "    'efficientnet-b1':2e-3,     # 1e-3: 82.53/93.9\n",
    "    'efficientnet-b2':1e-3,     # 1e-3, 2e-3     \n",
    "    'efficientnet-b3':2e-3,    \n",
    "    'efficientnet-b4':2e-3,  \n",
    "    'efficientnet-b5':1.5e-3,  \n",
    "    'efficientnet-b6':4e-3,\n",
    "    'efficientnet-b7':4e-3,\n",
    "    'efficientnet-lite0': 5e-4,\n",
    "    'efficientnet-lite2':1e-3,\n",
    "    'efficientnet-lite4':5e-4,\n",
    "    'resnet18':2.823e-02,\n",
    "    'resnet34':-1,\n",
    "    'resnet50':1.016e-02,\n",
    "    'resnet101':-1,\n",
    "    'resnet152':-1,\n",
    "})\n",
    "epochs = ({\n",
    "    'efficientnet-b0':(1,100), # for initally top losses\n",
    "    'efficientnet-b1':(6,20),\n",
    "    'efficientnet-b2':(15,50),\n",
    "    'efficientnet-b4':(5,25),\n",
    "    'efficientnet-lite0': (15,80),\n",
    "    'efficientnet-lite2': (5,25),\n",
    "    'resnet18':(10,50),\n",
    "    'resnet34':(10,50),\n",
    "    'resnet50':(10,50),\n",
    "    'resnet101':(10,50),\n",
    "    'resnet152':(10,50)\n",
    "})\n",
    "label_smoothing = ({           # a = 0.00     a = 0.05     a = 0.10     a = 0.15     a = 0.20\n",
    "    'efficientnet-b0':0.05,    #\n",
    "    'efficientnet-b1':0.05,    #\n",
    "    'efficientnet-b2':0.05,    # 0.05\n",
    "    'efficientnet-b3':0.05,    #\n",
    "    'efficientnet-b4':0.05,    #\n",
    "    'efficientnet-b5':0.05,\n",
    "    'efficientnet-b6':0.05,\n",
    "    'efficientnet-b7':0.05,\n",
    "    'efficientnet-lite0': 0.05,\n",
    "    'efficientnet-lite2': 0.05,\n",
    "    'resnet18':0.05,\n",
    "    'resnet34':0.05,\n",
    "    'resnet50':0.05,\n",
    "    'resnet101':0.05,\n",
    "    'resnet152':0.05\n",
    "})\n",
    "## Testing accuracies (top1,top5)\n",
    "scores = ({\n",
    "    'efficientnet-b0':(0,0),\n",
    "    'efficientnet-b1':(0,0),\n",
    "    'efficientnet-b2':(0,0),\n",
    "    'efficientnet-b3':(0,0),\n",
    "    'efficientnet-b4':(0,0),\n",
    "    'efficientnet-lite0':(0,0),\n",
    "    'efficientnet-lite2': (0,0),\n",
    "    'resnet50':(0,0)\n",
    "})\n",
    "tta_scores = ({\n",
    "    'efficientnet-b0':(0,0),\n",
    "    'efficientnet-b1':(0,0),\n",
    "    'efficientnet-b2':(0,0),\n",
    "    'efficientnet-b3':(0,0),\n",
    "    'efficientnet-b4':(0,0),\n",
    "    'efficientnet-lite0':(0,0),\n",
    "    'efficientnet-lite2': (0,0),\n",
    "    'efficientnet-lite4':(0,0),\n",
    "    'resnet50':(0,0)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Writing training data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.288929Z",
     "start_time": "2021-11-26T12:31:28.249683Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(dataset+\"_\"+noise_type+\"_\"+str(noise_ratio)+\".csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.Noisy_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viualization of Training Data, Samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "lbl = train_df['Label_numeric']\n",
    "# print(lbl)\n",
    "# print(lbl.size)\n",
    "plt.figure(figsize=(100,20))\n",
    "ax = sns.countplot(x= lbl, data=train_df)\n",
    "ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images Per Catergory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.304212Z",
     "start_time": "2021-11-26T12:31:28.290074Z"
    }
   },
   "outputs": [],
   "source": [
    "number_classes = train_df['Label'].nunique()\n",
    "print(\"Number of classes:\", number_classes)\n",
    "counted = train_df.groupby([\"Label\"]).size()\n",
    "print(counted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing testing  Data into CSV  (Test data frame df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.324789Z",
     "start_time": "2021-11-26T12:31:28.304863Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os.path\n",
    "test_dir = \"testing\"\n",
    "test_path = Path(test_dir)\n",
    "filepaths = list(test_path.glob(r'**/*.jpg'))\n",
    "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
    "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
    "labels = pd.Series(labels, name='Label')\n",
    "images = pd.concat([filepaths, labels], axis=1)\n",
    "test_df = pd.DataFrame(images)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Label'] = test_df['Label'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = test_df.sort_values(by='Label').reset_index(drop=True)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = sorted_df.Label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = {value: index for index, value in enumerate(my_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['Label_numeric'] = sorted_df['Label'].map(my_dict)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = sorted_df[['Filepath', 'Label_numeric', 'Label']]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = sorted_df['Label_numeric'].value_counts().sort_index().tolist()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted_df.Label_numeric.tolist()\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "lbl = test_df['Label_numeric']\n",
    "# print(lbl)\n",
    "# print(lbl.size)\n",
    "plt.figure(figsize=(70,20))\n",
    "ax = sns.countplot(x= lbl, data=test_df)\n",
    "ax.bar_label(ax.containers[0], weight = 'bold', fontsize = '30', color = 'red')\n",
    "plt.xticks(rotation=90, fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.yticks( fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.xlabel(\"Labels\", fontsize = 40, weight = 'bold', color = 'black')\n",
    "plt.ylabel(\"Number of Images\", fontsize = 40, weight = 'bold', color = 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Fastai and pytorch Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T12:31:28.609896Z",
     "start_time": "2021-11-26T12:31:28.325404Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai import __version__\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from fastai.vision import *\n",
    "from fastai import optimizer, losses, metrics\n",
    "from functools import partial, wraps\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fastai version checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "import torchvision\n",
    "print(fastai.__version__)\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch version checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.current_device()\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of data for learner (Fastai learner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tfms = [Zoom(),Rotate(), Flip(), Brightness(), Contrast(), Saturation()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# default augmentation\n",
    "bs   = batch_size[architecture]\n",
    "imgs = effnet_size[architecture]\n",
    "resize = (imgs*4)//3\n",
    "data0 = (ImageDataLoaders.from_df(train_df, valid_pct=0.2,bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms))\n",
    "\n",
    "print(\"Image size=\", imgs)\n",
    "print(\"Batch size=\", bs)\n",
    "print(\"Architecture=\", architecture)\n",
    "print(resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data0.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import damped_losses as losses\n",
    "loss_func = losses.DumpledSCELossFlat(num_classes=196, alpha = 0.1, beta = 1.0, delta=0.0, reduction='mean')\n",
    "model = EfficientNet.from_pretrained(architecture, num_classes=196)\n",
    "learner_type = Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pth = best_pth\n",
    "checkpoints = SaveModelCallback(fname=best_pth,monitor='accuracy',comp=np.greater, with_opt=True)\n",
    "# brainfreeze = BnFreeze()\n",
    "learn = ( learner_type(data0, model,metrics=[accuracy],loss_func=loss_func,\n",
    "                        cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "print(\"Best pth is=\", best_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot_lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_lr = 1e-4\n",
    "lr = 1e-3\n",
    "learn.fit_one_cycle(10, lr_max = slice(low_lr, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreeze the Learner\n",
    "fast.ai freezes the pre-trained part when we create the model. So up to this point, \n",
    "we only trained the last layer block. Next, we will unfreeze the pre-trained part and train the whole model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(50, lr_max = slice(low_lr, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = (ImageDataLoaders.from_df(test_df, valid_pct = 0.0, splitter=None, shuffle=False, label_col=1, item_tfms=Resize(imgs)))\n",
    "preds = learn.get_preds(dl=data_test)\n",
    "preds\n",
    "print(\"length of preds[1]\",len(preds[1]))\n",
    "acc= accuracy(preds[0], preds[1])\n",
    "print(\" BaselineTop-1 Accuracy:\", acc)\n",
    "# inter_test0 = ClassificationInterpretation.from_learner(tester0, dl =data_test)\n",
    "# inter_test0.plot_confusion_matrix(figsize = (50,60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ------------\n",
    " B0|Epochs|20|8056   |\n",
    " -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = {}\n",
    "acc = []\n",
    "log_preds, y  = learn.tta(dl=data_test)\n",
    "tta_acc = accuracy(log_preds, y)\n",
    "print(tta_acc)\n",
    "err[0] = (100.0, 100.0*(1.0-float(tta_acc)))\n",
    "acc.append(tta_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_test = ClassificationInterpretation.from_learner(learn, dl =data_test)\n",
    "inter_test.plot_confusion_matrix(figsize = (50,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='models/'+feature_path+'.p'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "import os\n",
    "# Specify the path of the folder where you want to create the directory\n",
    "folder_path = 'models'\n",
    "\n",
    "# Specify the name of the directory you want to create\n",
    "directory_name = 'baseline'\n",
    "\n",
    "# Combine the folder path and directory name\n",
    "new_directory_path = os.path.join(folder_path, directory_name)\n",
    "\n",
    "# Create the directory\n",
    "try:\n",
    "    # Create the directory\n",
    "    os.makedirs(new_directory_path)\n",
    "    print(f\"Directory '{directory_name}' created inside '{folder_path}'.\")\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{directory_name}' already exists inside '{folder_path}'.\")\n",
    "\n",
    "# Now, here we copy the best model for baseline and past into the destination, like the folder we created inside the model directory, named \"baseline\"!!!!!\n",
    "source = 'models/'+best_pth+'.pth'\n",
    "\n",
    "destination = 'models/baseline/'+best_pth+'.pth'\n",
    "# Copy the file from source to destination\n",
    "shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.vision.widgets import ImageClassifierCleaner\n",
    "import random as rn\n",
    "img_size = imgs\n",
    "drop_idxx = []\n",
    "los = []\n",
    "top_losses = []\n",
    "def class_frequencies(data,class_key='Label'):\n",
    "    classes = set(data[class_key])\n",
    "    class_freq = {}\n",
    "    max_freq = 0.5\n",
    "    min_freq =100000#1000000\n",
    "    for cl in classes:\n",
    "        class_df = data[data[class_key]==cl]\n",
    "        freq = len(class_df)\n",
    "        max_freq = max(freq,max_freq)\n",
    "        min_freq = min(freq,min_freq)\n",
    "        class_freq[cl] = freq\n",
    "    print(f'Max frequency is {max_freq}')\n",
    "    print(f'Min frequency is {min_freq}')\n",
    "    return class_freq,max_freq,min_freq\n",
    "    \n",
    "\n",
    "def russian_roulette(idx, losses, data,class_key='Label', min_prob=0.5,max_prob=0.9): #min_prob=0.5,max_prob=0.9 original values\n",
    "    \n",
    "    class_freq,max_freq,min_freq = class_frequencies(data,class_key)\n",
    "    min_prob = float(min_freq/max_freq)*max_prob\n",
    "    print(f'Min probability = {min_prob}')\n",
    "    deltap = (max_prob - min_prob)/(max_freq**2)\n",
    "\n",
    "    drop_idx = []\n",
    "    for n,i in enumerate(idx.numpy()):\n",
    "        l = data.iloc[i][class_key]\n",
    "        prob = min_prob + (class_freq[l]-min_freq)*deltap\n",
    "        r = rn.random()\n",
    "        if  r < prob:\n",
    "            drop_idx.append(i)\n",
    "            class_freq[l] -= 1 \n",
    "    return drop_idx\n",
    "\n",
    "def data_definition(data,img_size,tfms,model,best_pth,k_samples=100, min_prob=0.4, max_prob = 0.9, largest=True): # k_sample = 1000\n",
    "    databunch = ( ImageDataLoaders.from_df(data, valid_pct=0.0, bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms))\n",
    "    learn_cln = (learner_type(databunch, model,metrics=[accuracy],loss_func=loss_func, cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "    learn_cln.load(best_pth)\n",
    "    entire_training_set_csv0 = (ImageDataLoaders.from_df(data, valid_pct = 0.0, splitter=None, shuffle=False, label_col=1, item_tfms=Resize(imgs)))\n",
    "    interp = ClassificationInterpretation.from_learner(learn_cln, dl = entire_training_set_csv0)\n",
    "    losses = None\n",
    "    idx = None\n",
    "    if hybrid:\n",
    "        l_big,idx_big = interp.top_losses(k_samples//2,largest=True)\n",
    "        l_sm,idx_sm = interp.top_losses(k_samples//2,largest=False)\n",
    "        losses = torch.cat((l_big,l_sm),0)\n",
    "        idx = torch.cat((idx_big,idx_sm),0)\n",
    "    else:\n",
    "        losses,idx = interp.top_losses(k_samples,largest=largest)\n",
    "    drop_idx = russian_roulette(idx,losses,data,min_prob = min_prob, max_prob = max_prob)\n",
    "    data_filtered = data.drop(data.index[drop_idx])\n",
    "    print(f'Dropped {len(drop_idx)} top losses')\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Damped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drop_idxx = []\n",
    "los = []\n",
    "top_losses = []\n",
    "n_training_series = 15  # orginally it is 15, but for testing purpose we are using with 1 to check the scheme with custom function.\n",
    "min_epochs = 1\n",
    "min_prob=0.5\n",
    "max_prob=0.9\n",
    "delta_epochs_per_serie =  0\n",
    "min_lr = 1e-4\n",
    "kappa_lr = 1 \n",
    "k_samples = 0\n",
    "data_per_serie = {}\n",
    "data_per_serie[0] = train_df\n",
    "largest = True\n",
    "from_scratch = False\n",
    "hybrid = False\n",
    "cleaning_cycles = 3 \n",
    "delta_max = 0.25\n",
    "accs = []\n",
    "deltas = []\n",
    "for c in range(cleaning_cycles):\n",
    "    for n in range(n_training_series):\n",
    "        \n",
    "        delta =  delta_max * math.sin( math.pi * float(n+1)/(n_training_series) )\n",
    "        loss_func = losses.DumpledSCELossFlat(num_classes=196, alpha = 0.1, beta = 1.0, delta=delta, reduction='mean')\n",
    "        k_samples = 50\n",
    "        epochs_per_serie = min_epochs + n * delta_epochs_per_serie\n",
    "        lr = min_lr * kappa_lr * (n_training_series - n)\n",
    "        data_per_serie[n+1] = data_definition(data_per_serie[n],img_size,batch_tfms,model,best_pth,k_samples=k_samples, \n",
    "                                              min_prob = min_prob, max_prob=max_prob, largest = largest)\n",
    "        print(\"************Round: \",n)\n",
    "        print(\"************data per serie:******************\")\n",
    "        print(\"************length of data per serie:******************\")\n",
    "        print(len(data_per_serie[n+1]))\n",
    "        databunch = ( ImageDataLoaders.from_df(data_per_serie[n+1], valid_pct = 0.2,bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms))\n",
    "        learn =(learner_type(databunch, model,metrics=[accuracy],loss_func=loss_func,\n",
    "                            cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "        learn.unfreeze() \n",
    "        learn.load(best_pth)\n",
    "        learn.fit_one_cycle(epochs_per_serie, lr_max=slice(1e-6, 5e-5))\n",
    "        if n%3== 0:\n",
    "            log_preds, y = learn.tta(dl=data_test)\n",
    "            print(\"************Round: \",n)\n",
    "            tta_acc = accuracy(log_preds, y)\n",
    "            print(f'TTA Accuracy:{tta_acc}')\n",
    "            acc.append(tta_acc)\n",
    "            accs.append(tta_acc)\n",
    "            deltas.append(delta)\n",
    "            err[c*n_training_series + n] = ( 100.0 *float(1.0 - len(data_per_serie[n+1])/len(data_per_serie[0])) ,  100.0 * (1.0 - float(tta_acc)))\n",
    "            print(f'cycle {c} Error Rate is [{err}]')\n",
    "#             inter_test = ClassificationInterpretation.from_learner(learn, dl =data_test)\n",
    "#             inter_test.plot_confusion_matrix(figsize = (10,10))\n",
    "            PATH='models/'+dump_1_features+'_'+str(c)+\"_\"+str(n)+\".p\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            csv = data_per_serie[n+1].to_csv('CSVz/'+dump_1_csvs+'_'+str(c)+\"_\"+str(n)+\".csv\", index = False)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err = {}\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(accs)):\n",
    "    print(f'acc:{accs[i]}, delta:{deltas[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(accs, label='Accuracy', color='tab:blue', marker='o')\n",
    "plt.plot(deltas, label='Delta', color='tab:red', marker='o')\n",
    "for i, (acc, delta) in enumerate(zip(accs, deltas)):\n",
    "    plt.text(i, acc, f\"{acc:.4f}\", ha='center', va='bottom', fontsize=8, rotation=45, color='black', fontweight='bold')\n",
    "    plt.text(i, delta, f\"{delta:.4f}\", ha='center', va='top', fontsize=8, rotation=45, color='black', fontweight='bold')\n",
    "plt.title('Comparison of Accuracy and Delta Values', fontweight='bold', color = 'Black')\n",
    "plt.xlabel('Training Series', fontweight='bold', color = 'black')\n",
    "plt.ylabel('Delta Values and Accuracy (%)', fontweight='bold', color = 'Black')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()  # Adjust layout to prevent clipping of annotations\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump 0.50 with cleaning Scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('baseline/'+best_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idxx = []\n",
    "los = []\n",
    "top_losses = []\n",
    "n_training_series = 15\n",
    "min_epochs = 1\n",
    "min_prob=0.5\n",
    "max_prob=0.9\n",
    "delta_epochs_per_serie =  0\n",
    "min_lr = 1e-4\n",
    "kappa_lr = 1 \n",
    "k_samples = 0\n",
    "data_per_serie = {}\n",
    "data_per_serie[0] = train_df\n",
    "largest = True\n",
    "from_scratch = False\n",
    "hybrid = False\n",
    "cleaning_cycles = 3 \n",
    "delta_max = 0.50\n",
    "accs = []\n",
    "deltas = []\n",
    "err = {}\n",
    "acc = []\n",
    "for c in range(cleaning_cycles):\n",
    "    for n in range(n_training_series):\n",
    "        \n",
    "        delta =  delta_max * math.sin( math.pi * float(n+1)/(n_training_series) )\n",
    "        loss_func = losses.DumpledSCELossFlat(num_classes=196, alpha = 0.1, beta = 1.0, delta=delta, reduction='mean')\n",
    "        k_samples = 50\n",
    "        print(f'+++++ Training serie: inverse pyramid {n} ++++++++')\n",
    "        epochs_per_serie = min_epochs + n * delta_epochs_per_serie\n",
    "        lr = min_lr * kappa_lr * (n_training_series - n)\n",
    "        data_per_serie[n+1] = data_definition(data_per_serie[n],img_size,batch_tfms,model,best_pth,k_samples=k_samples, \n",
    "                                              min_prob = min_prob, max_prob=max_prob, largest = largest)\n",
    "        print(\"************Round: \",n)\n",
    "        print(\"************data per serie:******************\")\n",
    "        print(\"************length of data per serie:******************\")\n",
    "        print(len(data_per_serie[n+1]))\n",
    "        databunch = ( ImageDataLoaders.from_df(data_per_serie[n+1], valid_pct = 0.2,bs=bs,label_col=1,\n",
    "                                        shuffle_train=True,\n",
    "                                        item_tfms=Resize(imgs),\n",
    "                                        batch_tfms = batch_tfms))\n",
    "        learn =(learner_type(databunch, model,metrics=[accuracy],loss_func=loss_func,\n",
    "                            cbs=[ShowGraphCallback(),checkpoints]).to_fp16())\n",
    "        learn.unfreeze() \n",
    "        learn.load(best_pth)\n",
    "        learn.fit_one_cycle(epochs_per_serie, lr_max=slice(1e-6, 5e-5))\n",
    "        if n%3== 0:\n",
    "            log_preds, y = learn.tta(dl=data_test)\n",
    "            print(\"************Round: \",n)\n",
    "            tta_acc = accuracy(log_preds, y)\n",
    "            print(f'TTA Accuracy:{tta_acc}')\n",
    "            acc.append(tta_acc)\n",
    "            accs.append(tta_acc)\n",
    "            deltas.append(delta)\n",
    "            err[c*n_training_series + n] = ( 100.0 *float(1.0 - len(data_per_serie[n+1])/len(data_per_serie[0])) ,  100.0 * (1.0 - float(tta_acc)))\n",
    "            print(f'cycle {c} Error Rate is [{err}]')\n",
    "            inter_test = ClassificationInterpretation.from_learner(learn, dl =data_test)\n",
    "            inter_test.plot_confusion_matrix(figsize = (10,10))\n",
    "            PATH='models/'+dump_2_features+'_'+str(c)+\"_\"+str(n)+\".p\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            csv = data_per_serie[n+1].to_csv('CSVz/'+dump_2_csvs+'_'+str(c)+\"_\"+str(n)+\".csv\", index = False)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err = {}\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(accs)):\n",
    "    print(f'acc:{accs[i]}, delta:{deltas[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(accs, label='Accuracy', color='tab:blue', marker='o')\n",
    "plt.plot(deltas, label='Delta', color='tab:red', marker='o')\n",
    "for i, (acc, delta) in enumerate(zip(accs, deltas)):\n",
    "    plt.text(i, acc, f\"{acc:.4f}\", ha='center', va='bottom', fontsize=8, rotation=45, color='black', fontweight='bold')\n",
    "    plt.text(i, delta, f\"{delta:.4f}\", ha='center', va='top', fontsize=8, rotation=45, color='black', fontweight='bold')\n",
    "\n",
    "plt.title('Comparison of Accuracy and Delta Values', fontweight='bold', color = 'Black')\n",
    "plt.xlabel('Training Series', fontweight='bold', color = 'black')\n",
    "plt.ylabel('Delta Values and Accuracy (%)', fontweight='bold', color = 'Black')\n",
    "plt.legend()\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Food101_Kaggle.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
